import torch
import torch.nn as nn
import argparse
import os
import numpy as np
from models.SSM2Mel import Decoder
from util.dataset import RegressionDataset
import glob

def load_model(model_path, device, **model_args):
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œè‡ªåŠ¨æ£€æµ‹ä»»åŠ¡æ¨¡å¼"""
    # é¦–å…ˆå°è¯•åŠ è½½æ£€æŸ¥ç‚¹ä»¥æ£€æµ‹ä»»åŠ¡æ¨¡å¼
    checkpoint = torch.load(model_path, map_location=device)
    
    # æ£€æµ‹æ£€æŸ¥ç‚¹ä¸­çš„ä»»åŠ¡æ¨¡å¼
    fc_weight_shape = checkpoint['fc.weight'].shape
    if fc_weight_shape[0] == 1:
        detected_task_mode = "envelope"
        print(f"ğŸ” æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ä»»åŠ¡æ¨¡å¼: éŸ³é¢‘åŒ…ç»œé‡å»º (è¾“å‡ºç»´åº¦: 1)")
    elif fc_weight_shape[0] == 80:
        detected_task_mode = "mel_spectrogram"
        print(f"ğŸ” æ£€æµ‹åˆ°æ£€æŸ¥ç‚¹ä»»åŠ¡æ¨¡å¼: å®Œæ•´Melé¢‘è°±é‡å»º (è¾“å‡ºç»´åº¦: 80)")
    else:
        detected_task_mode = "mel_spectrogram"  # é»˜è®¤å‡è®¾ä¸ºå¤šé¢‘å¸¦
        print(f"âš ï¸  æœªçŸ¥çš„è¾“å‡ºç»´åº¦ {fc_weight_shape[0]}ï¼Œå‡è®¾ä¸ºå®Œæ•´Melé¢‘è°±é‡å»ºæ¨¡å¼")
    
    # æ›´æ–°æ¨¡å‹å‚æ•°ä»¥åŒ¹é…æ£€æŸ¥ç‚¹
    model_args['task_mode'] = detected_task_mode
    
    # åˆ›å»ºæ¨¡å‹
    model = Decoder(**model_args).to(device)
    
    # åŠ è½½æ£€æŸ¥ç‚¹
    model.load_state_dict(checkpoint)
    model.eval()
    
    print(f"âœ… æˆåŠŸåŠ è½½æ¨¡å‹ï¼Œä»»åŠ¡æ¨¡å¼: {detected_task_mode}")
    return model

def inference_single_sample(model, eeg_data, sub_id, device):
    """å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œæ¨ç†"""
    model.eval()
    with torch.no_grad():
        # ç¡®ä¿è¾“å…¥æ ¼å¼æ­£ç¡®
        if len(eeg_data.shape) == 2:  # [T, C]
            eeg_data = eeg_data.unsqueeze(0)  # [1, T, C]
        
        eeg_data = eeg_data.to(device)
        sub_id = torch.tensor([sub_id]).to(device)
        
        # è¿›è¡Œæ¨ç†
        output = model(eeg_data, sub_id)
        return output.squeeze()

def inference_on_dataset(model, data_loader, device):
    """å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ¨ç†"""
    model.eval()
    all_outputs = []
    all_labels = []
    
    with torch.no_grad():
        for inputs, labels, sub_id in data_loader:
            inputs = inputs.squeeze(0).to(device)
            labels = labels.squeeze(0).to(device)
            sub_id = sub_id.to(device)
            
            outputs = model(inputs, sub_id)
            
            all_outputs.append(outputs.cpu().numpy())
            all_labels.append(labels.cpu().numpy())
    
    return np.concatenate(all_outputs, axis=0), np.concatenate(all_labels, axis=0)

def main():
    parser = argparse.ArgumentParser(description='SSM2Mel Inference')
    parser.add_argument('--model_path', type=str, 
                       default='/home/binwen6/code/CBD/SSM2Mel/result_model_conformer/model_epoch150.pt',
                       help='Path to the trained model')
    parser.add_argument('--data_folder', type=str, 
                       default='/home/binwen6/code/CBD/SSM2Mel/data/split_data',
                       help='Path to the test data folder')
    parser.add_argument('--output_folder', type=str, 
                       default='/home/binwen6/code/CBD/SSM2Mel/inference_results',
                       help='Path to save inference results')
    parser.add_argument('--gpu', type=int, default=0, help='GPU device ID')
    parser.add_argument('--batch_size', type=int, default=1, help='Batch size for inference')
    
    # æ¨¡å‹å‚æ•°ï¼ˆéœ€è¦ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    parser.add_argument('--win_len', type=int, default=10)
    parser.add_argument('--sample_rate', type=int, default=64)
    parser.add_argument('--in_channel', type=int, default=64)
    parser.add_argument('--d_model', type=int, default=64)
    parser.add_argument('--d_inner', type=int, default=1024)
    parser.add_argument('--n_head', type=int, default=2)
    parser.add_argument('--n_layers', type=int, default=1)
    parser.add_argument('--fft_conv1d_kernel', type=tuple, default=(9, 1))
    parser.add_argument('--fft_conv1d_padding', type=tuple, default=(4, 0))
    parser.add_argument('--dropout', type=float, default=0.5)
    parser.add_argument('--g_con', default=True)
    parser.add_argument('--task_mode', type=str, default="mel_spectrogram", 
                       choices=["envelope", "mel_spectrogram"], 
                       help="ä»»åŠ¡æ¨¡å¼: envelope(éŸ³é¢‘åŒ…ç»œé‡å»º) æˆ– mel_spectrogram(å®Œæ•´Melé¢‘è°±é‡å»º)")
    
    args = parser.parse_args()
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device(f"cuda:{args.gpu}" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    os.makedirs(args.output_folder, exist_ok=True)
    
    # å‡†å¤‡æ¨¡å‹å‚æ•°
    model_args = {
        'in_channel': args.in_channel,
        'd_model': args.d_model,
        'd_inner': args.d_inner,
        'n_head': args.n_head,
        'n_layers': args.n_layers,
        'fft_conv1d_kernel': args.fft_conv1d_kernel,
        'fft_conv1d_padding': args.fft_conv1d_padding,
        'dropout': args.dropout,
        'g_con': args.g_con,
        'task_mode': args.task_mode  # æ·»åŠ ä»»åŠ¡æ¨¡å¼å‚æ•°
    }
    
    # åŠ è½½æ¨¡å‹
    print(f"Loading model from {args.model_path}")
    model = load_model(args.model_path, device, **model_args)
    print("Model loaded successfully!")
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    input_length = args.sample_rate * args.win_len
    features = ["eeg"] + ["mel"]
    
    # è·å–æµ‹è¯•æ–‡ä»¶
    test_files = [x for x in glob.glob(os.path.join(args.data_folder, "test_-_*")) 
                  if os.path.basename(x).split("_-_")[-1].split(".")[0] in features]
    
    if not test_files:
        print("No test files found!")
        return
    
    print(f"Found {len(test_files)} test files")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    test_set = RegressionDataset(test_files, input_length, args.in_channel, 'test', args.g_con)
    test_dataloader = torch.utils.data.DataLoader(
        test_set,
        batch_size=args.batch_size,
        num_workers=4,
        sampler=None,
        drop_last=True,
        shuffle=False
    )
    
    # è¿›è¡Œæ¨ç†
    print("Starting inference...")
    outputs, labels = inference_on_dataset(model, test_dataloader, device)
    
    # ä¿å­˜ç»“æœ
    output_file = os.path.join(args.output_folder, 'inference_results.npz')
    np.savez(output_file, outputs=outputs, labels=labels)
    print(f"Inference results saved to {output_file}")
    
    # è®¡ç®—ä¸€äº›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
    print(f"Output shape: {outputs.shape}")
    print(f"Labels shape: {labels.shape}")
    print(f"Output range: [{outputs.min():.4f}, {outputs.max():.4f}]")
    print(f"Labels range: [{labels.min():.4f}, {labels.max():.4f}]")
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    from util.cal_pearson import pearson_metric
    correlation = pearson_metric(torch.tensor(outputs), torch.tensor(labels))
    print(f"Pearson correlation: {correlation.mean().item():.4f}")

if __name__ == '__main__':
    main() 